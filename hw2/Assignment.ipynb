{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4506266b-8594-4e75-9aa9-e3a90908cd4a",
      "metadata": {
        "id": "4506266b-8594-4e75-9aa9-e3a90908cd4a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "from torch.nn import LayerNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0176a9a9-bef8-42d9-8648-647bea9b98bd",
      "metadata": {
        "collapsed": true,
        "id": "0176a9a9-bef8-42d9-8648-647bea9b98bd",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: thop in ./venv/lib/python3.12/site-packages (0.1.1.post2209072238)\n",
            "Requirement already satisfied: torch in ./venv/lib/python3.12/site-packages (from thop) (2.9.1)\n",
            "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch->thop) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.12/site-packages (from torch->thop) (4.15.0)\n",
            "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch->thop) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.12/site-packages (from torch->thop) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in ./venv/lib/python3.12/site-packages (from torch->thop) (3.5)\n",
            "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch->thop) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in ./venv/lib/python3.12/site-packages (from torch->thop) (2025.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch->thop) (3.0.3)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: tdqm in ./venv/lib/python3.12/site-packages (0.0.1)\n",
            "Requirement already satisfied: tqdm in ./venv/lib/python3.12/site-packages (from tdqm) (4.67.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install thop\n",
        "!pip install tdqm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p6Nuv16qsHiw",
      "metadata": {
        "id": "p6Nuv16qsHiw"
      },
      "source": [
        "# Part 1: Attention and Encoder-Decoder Models\n",
        "\n",
        "In this section, you'll implement two forms of attention and use them in an encoder-decoder style transformer. You'll then compare performance based on changing dataset size and hidden dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "owXPYu9s0YDY",
      "metadata": {
        "id": "owXPYu9s0YDY"
      },
      "source": [
        "## Step 1: Scaled Dot Product Attention\n",
        "Implement both the base `scaled_dot_attention` method as well as the `forward` method for the `Attention` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "5702fa5e-9df0-486f-8aba-6c73285683b0",
      "metadata": {
        "id": "5702fa5e-9df0-486f-8aba-6c73285683b0"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_attention(q, k, v, mask=0):\n",
        "  \"\"\"Computes scaled dot product attention with an optional mask.\n",
        "    q : shape ( batch, k, hidden_size)\n",
        "    k : shape ( batch, seq_len, hidden_size)\n",
        "    v : shape ( batch, seq_len, hidden_size)\n",
        "    mask: optional. shape (k, seq_len)\n",
        "\n",
        "  returns:\n",
        "      context   : shape (batch, k, hidden_size)\n",
        "      attention : shape (batch, k, seq_len)\n",
        "  \"\"\"\n",
        "\n",
        "  # ------------\n",
        "  # FILL THIS IN\n",
        "  # ------------\n",
        "  # unnorm_attn = \n",
        "  # masked_unnorm_attn =\n",
        "  # attention =\n",
        "  # context =\n",
        "  unnorm_attn = (q @ k.transpose(-2, -1)) / (q.size(-1) ** 0.5)\n",
        "  masked_unnorm_attn = unnorm_attn + mask\n",
        "  attention = torch.nn.functional.softmax(masked_unnorm_attn, dim=-1)\n",
        "  context = attention @ v\n",
        "\n",
        "  return context, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "32fc86ce-2033-4c7f-be85-bdbde2d0db01",
      "metadata": {
        "id": "32fc86ce-2033-4c7f-be85-bdbde2d0db01"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, x, annots):\n",
        "        #   x         : shape ( batch, k, hidden_size)\n",
        "        #   annots    : shape ( batch, seq_len, hidden_size)\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # Pay attention to which inputs you use for the queries, keys, and values.\n",
        "        # ------------\n",
        "        # q =\n",
        "        # k =\n",
        "        # v =\n",
        "        q = self.Q(x)\n",
        "        k = self.K(annots)\n",
        "        v = self.V(annots)\n",
        "\n",
        "        return scaled_dot_attention(q, k, v)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "moGWt3y107-Q",
      "metadata": {
        "id": "moGWt3y107-Q"
      },
      "source": [
        "## Step 2: Causal Scaled Dot Product Attention\n",
        "Using your `scaled_dot_attention` method, implement the `forward` method for the `CausalAttention` module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "90e5ef29-cb04-4b24-ac86-c5abf91c3fc1",
      "metadata": {
        "id": "90e5ef29-cb04-4b24-ac86-c5abf91c3fc1"
      },
      "outputs": [],
      "source": [
        "class CausalAttention(Attention):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__(hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #   x : shape ( batch, seq_len, hidden_size)\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # This should be very similar to your other attention implementation,\n",
        "        # with the exception that you only have one set of outputs and you\n",
        "        # must construct a causal attention mask.\n",
        "        # ------------\n",
        "        # q =\n",
        "        # k =\n",
        "        # v =\n",
        "        # mask =\n",
        "        q = self.Q(x)\n",
        "        k = self.K(x)\n",
        "        v = self.V(x)\n",
        "        seq_len = x.size(1)\n",
        "        mask = torch.triu(torch.full((seq_len, seq_len), float('-inf'), device=x.device), diagonal=1)\n",
        "        return scaled_dot_attention(q, k, v, mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71055b34-ad1a-4384-b236-1e596e36e968",
      "metadata": {
        "id": "71055b34-ad1a-4384-b236-1e596e36e968"
      },
      "source": [
        "#### Simple test cases\n",
        "##### Passing this DOES NOT mean your implementation is correct. But, if this fails, then you definitely did something wrong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "6f88e426-84f6-4d14-ac20-8922b8a63685",
      "metadata": {
        "id": "6f88e426-84f6-4d14-ac20-8922b8a63685"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking consistent shape\n",
            "Checking attention mask\n",
            "Passed\n"
          ]
        }
      ],
      "source": [
        "cros_attn = Attention(32)\n",
        "caus_attn = CausalAttention(32)\n",
        "caus_attn.Q = cros_attn.Q\n",
        "caus_attn.K = cros_attn.K\n",
        "caus_attn.V = cros_attn.V\n",
        "\n",
        "inp = torch.randn(4, 8, 32)\n",
        "inp2 = torch.randn(4, 16, 32)\n",
        "\n",
        "print(\"Checking consistent shape\")\n",
        "assert cros_attn(inp, inp2)[0].shape == inp.shape\n",
        "assert caus_attn(inp)[0].shape == inp.shape\n",
        "assert cros_attn(inp, inp2)[1].shape == (4, 8, 16)\n",
        "assert caus_attn(inp)[1].shape == (4, 8, 8)\n",
        "\n",
        "print(\"Checking attention mask\")\n",
        "assert torch.all(caus_attn(inp)[0][:, -1] == cros_attn(inp, inp)[0][:, -1])\n",
        "\n",
        "print(\"Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf0eab58-2e6b-40c8-ab7d-f53782a90cc0",
      "metadata": {
        "id": "bf0eab58-2e6b-40c8-ab7d-f53782a90cc0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b510c87b-cef7-45fc-9c93-2e1c9db32332",
      "metadata": {
        "id": "b510c87b-cef7-45fc-9c93-2e1c9db32332"
      },
      "source": [
        "#### Read through the following code. Make sure you understand what is going on.\n",
        "This code sets up the encoder-decoder architecture. It requires no additional code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "8ba5f837-3737-464e-8fc0-2c74319f739d",
      "metadata": {
        "id": "8ba5f837-3737-464e-8fc0-2c74319f739d"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, hidden_size, max_len = 1000):\n",
        "        super().__init__()\n",
        "\n",
        "        pos = torch.arange(max_len).float().unsqueeze(1)\n",
        "        dim = torch.arange(hidden_size // 2).float().unsqueeze(0)\n",
        "\n",
        "        div_term = torch.exp(-math.log(10000.0) * (2 * dim) / hidden_size)\n",
        "        angle = pos * div_term\n",
        "\n",
        "        pe = torch.zeros(max_len, hidden_size)\n",
        "        pe[:, 0::2] = torch.sin(angle)\n",
        "        pe[:, 1::2] = torch.cos(angle)\n",
        "\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        return self.pe[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "96913cd5-a3b4-47cf-84c4-3ed6a7219dca",
      "metadata": {
        "id": "96913cd5-a3b4-47cf-84c4-3ed6a7219dca"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.layer1  = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.layer2  = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.relu    = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.norm1 = LayerNorm(hidden_size)\n",
        "        self.norm2 = LayerNorm(hidden_size)\n",
        "        self.attn = Attention(hidden_size)\n",
        "        self.mlp = MLP(hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        context, attention = self.attn(self.norm1(x), self.norm1(x))\n",
        "        x = x + context\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x, attention\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.norm1 = LayerNorm(hidden_size)\n",
        "        self.norm2 = LayerNorm(hidden_size)\n",
        "        self.cros_attn = Attention(hidden_size)\n",
        "        self.self_attn = CausalAttention(hidden_size)\n",
        "        self.mlp = MLP(hidden_size)\n",
        "\n",
        "    def forward(self, x, annotation):\n",
        "        context, self_attn = self.self_attn(self.norm1(x))\n",
        "        x = x + context\n",
        "        context, cros_attn = self.cros_attn(self.norm2(x), annotation)\n",
        "        x = x + context\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x, self_attn, cros_attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "6580b9e1-71d0-42f5-a29d-44eb2754bc01",
      "metadata": {
        "id": "6580b9e1-71d0-42f5-a29d-44eb2754bc01"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class TransformerEncoderDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tok_emb = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.pos_emb = PositionalEncoding(hidden_size)\n",
        "\n",
        "        self.encoder_blocks = nn.ModuleList(\n",
        "            [EncoderBlock(hidden_size) for i in range(num_layers)]\n",
        "        )\n",
        "        self.decoder_blocks = nn.ModuleList(\n",
        "            [DecoderBlock(hidden_size) for i in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.head = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "        self.head.weight = self.tok_emb.weight\n",
        "        # weight tie, the final layer reuses the weights of the embedding layers\n",
        "        # this reduces the total number of parameters\n",
        "\n",
        "        self.apply(self._init_weights) # initialize the weights to a small number\n",
        "\n",
        "    def forward(self, inputs, annots):\n",
        "\n",
        "        pos_x = torch.arange(0, inputs.shape[-1], device=inputs.device).long() # creates tensor([0, 1, 2, 3, ...])\n",
        "        pos_y = torch.arange(0, annots.shape[-1], device=inputs.device).long() # creates tensor([0, 1, 2, 3, ...])\n",
        "\n",
        "        x = self.tok_emb(inputs) + self.pos_emb(pos_x)\n",
        "        y = self.tok_emb(annots) + self.pos_emb(pos_y)\n",
        "\n",
        "        self_attn = []\n",
        "        cros_attn = []\n",
        "\n",
        "        for encode in self.encoder_blocks:\n",
        "            y, _ = encode(y)\n",
        "\n",
        "        for decode in self.decoder_blocks:\n",
        "            x, s, c = decode(x, y)\n",
        "            cros_attn.append(c)\n",
        "            self_attn.append(s)\n",
        "\n",
        "        return self.head(x), {'self_attn': self_attn, 'cros_attn': cros_attn}\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c6a18d8-e4c7-49fe-b2a9-a42ceb1b1134",
      "metadata": {
        "id": "0c6a18d8-e4c7-49fe-b2a9-a42ceb1b1134"
      },
      "source": [
        "## Step 3: Training the model\n",
        "\n",
        "You'll now train a set of example models. This section requires no additional code from you, with the exception of setting the datapath. However, **you are required to analyze and include some of its output**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "95cf818b-e671-435c-b4cc-1646a6768b14",
      "metadata": {
        "id": "95cf818b-e671-435c-b4cc-1646a6768b14"
      },
      "outputs": [],
      "source": [
        "from dataset import EncoderDecoderDataset, DecoderDataset\n",
        "from utils import save_loss_comparison_by_dataset, save_loss_comparison_by_hidden\n",
        "from torch.utils.data import DataLoader\n",
        "from trainer import train\n",
        "import torch.optim as optim\n",
        "\n",
        "data_path = './' # Change data_path to where the data is stored\n",
        "\n",
        "encdec_ds_small = EncoderDecoderDataset(data_path, 'pig_latin_small')\n",
        "encdec_ds_large = EncoderDecoderDataset(data_path, 'pig_latin_large')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "8c469981-d5ea-4ce0-91b9-6882dd45053c",
      "metadata": {
        "id": "8c469981-d5ea-4ce0-91b9-6882dd45053c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset sample:\n",
            "Input: <SOS>elfareway\n",
            "Annotation: welfare\n",
            "Target: elfareway<EOS>\n"
          ]
        }
      ],
      "source": [
        "print(\"Dataset sample:\")\n",
        "print(\"Input:\", encdec_ds_small.to_token(encdec_ds_small[0][0].tolist()))\n",
        "print(\"Annotation:\", encdec_ds_small.to_token(encdec_ds_small[0][1].tolist()))\n",
        "print(\"Target:\", encdec_ds_small.to_token(encdec_ds_small[0][2].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "4f9201b2-bce8-4b1d-bd19-07a1d871743d",
      "metadata": {
        "id": "4f9201b2-bce8-4b1d-bd19-07a1d871743d",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/dylan/home/columbia/neural_networks/hw2/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train loss: 2.606605 | Val loss 2.254194\n",
            "Epoch 2 | Train loss: 2.120210 | Val loss 1.967676\n",
            "Epoch 3 | Train loss: 1.876924 | Val loss 1.744096\n",
            "Epoch 4 | Train loss: 1.589098 | Val loss 1.402211\n",
            "Epoch 5 | Train loss: 1.209611 | Val loss 1.098448\n",
            "Epoch 6 | Train loss: 0.858890 | Val loss 0.628684\n",
            "Epoch 7 | Train loss: 0.553120 | Val loss 0.473719\n",
            "Epoch 8 | Train loss: 0.368946 | Val loss 0.285661\n",
            "Epoch 9 | Train loss: 0.268761 | Val loss 0.278207\n",
            "Epoch 10 | Train loss: 0.248829 | Val loss 0.184922\n",
            "Epoch 11 | Train loss: 0.175198 | Val loss 0.163086\n",
            "Epoch 12 | Train loss: 0.161787 | Val loss 0.140198\n",
            "Epoch 13 | Train loss: 0.134959 | Val loss 0.118508\n",
            "Epoch 14 | Train loss: 0.109624 | Val loss 0.104115\n",
            "Epoch 15 | Train loss: 0.101698 | Val loss 0.103861\n",
            "Epoch 16 | Train loss: 0.106461 | Val loss 0.098093\n",
            "Epoch 17 | Train loss: 0.075885 | Val loss 0.083083\n",
            "Epoch 18 | Train loss: 0.065770 | Val loss 0.077757\n",
            "Epoch 19 | Train loss: 0.062860 | Val loss 0.087378\n",
            "Epoch 20 | Train loss: 0.058537 | Val loss 0.061310\n",
            "Epoch 21 | Train loss: 0.047119 | Val loss 0.070654\n",
            "Epoch 22 | Train loss: 0.047980 | Val loss 0.088414\n",
            "Epoch 23 | Train loss: 0.042670 | Val loss 0.063974\n",
            "Epoch 24 | Train loss: 0.040459 | Val loss 0.060575\n",
            "Epoch 25 | Train loss: 0.035871 | Val loss 0.064396\n"
          ]
        }
      ],
      "source": [
        "model = TransformerEncoderDecoder(\n",
        "    vocab_size = len(encdec_ds_small.idx_to_token),\n",
        "    hidden_size = 16,\n",
        "    num_layers = 4\n",
        ")\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-2)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "results = train(\n",
        "    model=model,\n",
        "    dataset=encdec_ds_small,\n",
        "    num_epochs=25,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    device=device,\n",
        "    batch_size = 32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "8ee60862-a569-41bc-b4c3-cb22b693c2ea",
      "metadata": {
        "id": "8ee60862-a569-41bc-b4c3-cb22b693c2ea"
      },
      "outputs": [],
      "source": [
        "def encdec_generate(input, model, dataset, max_len=20):\n",
        "    gen_string = \"\"\n",
        "    idx = torch.tensor([[dataset.start_idx]]).to(device)\n",
        "    annots = torch.tensor([dataset.to_idx(input) + [dataset.end_idx]*(max(0, 12-len(input)))]).to(device)\n",
        "    #pad the input string with <EOS> since the collate_fn had to pad the stirngs\n",
        "\n",
        "    for i in range(max_len):\n",
        "        logits = model(idx, annots)[0]\n",
        "\n",
        "        token_idx = logits[0, -1:].argmax(dim=-1)\n",
        "        idx = torch.cat([idx, token_idx.unsqueeze(0)], dim=1)\n",
        "\n",
        "        if token_idx.item() == dataset.end_idx:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = gen_string + dataset.idx_to_token[token_idx.item()]\n",
        "    return gen_string\n",
        "\n",
        "def encdec_translate(sentence, model, dataset):\n",
        "    words = sentence.split()\n",
        "    return \" \".join([encdec_generate(w, model, dataset) for w in words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "68758325-5f25-4ddd-bf8b-dd4859803c99",
      "metadata": {
        "id": "68758325-5f25-4ddd-bf8b-dd4859803c99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ethay'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encdec_translate('the', model, encdec_ds_small)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "9135ea7f-9665-4aba-a53e-08ec9c56d40d",
      "metadata": {
        "id": "9135ea7f-9665-4aba-a53e-08ec9c56d40d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ethay airway onditioninggay isway orkingway'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encdec_translate(\"the air conditioning is working\", model, encdec_ds_small)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "d902930a-4aff-43da-8920-ba705afd9c76",
      "metadata": {
        "id": "d902930a-4aff-43da-8920-ba705afd9c76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'estingtay omesay andomray ordsway'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encdec_translate(\"testing some random words\", model, encdec_ds_small)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RKpvQTmQ7nnn",
      "metadata": {
        "id": "RKpvQTmQ7nnn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training model: hidden_size=16, dataset=small\n",
            "Epoch 1 | Train loss: 2.582781 | Val loss 2.354807\n",
            "Epoch 2 | Train loss: 2.165793 | Val loss 2.030854\n",
            "Epoch 3 | Train loss: 1.926766 | Val loss 1.791198\n",
            "Epoch 4 | Train loss: 1.633937 | Val loss 1.433014\n",
            "Epoch 5 | Train loss: 1.162860 | Val loss 0.979108\n",
            "Epoch 6 | Train loss: 0.824072 | Val loss 0.607942\n",
            "Epoch 7 | Train loss: 0.557771 | Val loss 0.421024\n",
            "Epoch 8 | Train loss: 0.382473 | Val loss 0.329119\n",
            "Epoch 9 | Train loss: 0.294242 | Val loss 0.312277\n",
            "Epoch 10 | Train loss: 0.272227 | Val loss 0.209310\n",
            "Epoch 11 | Train loss: 0.207903 | Val loss 0.183759\n",
            "Epoch 12 | Train loss: 0.157604 | Val loss 0.171518\n",
            "Epoch 13 | Train loss: 0.166664 | Val loss 0.132480\n",
            "Epoch 14 | Train loss: 0.109757 | Val loss 0.112500\n",
            "Epoch 15 | Train loss: 0.104929 | Val loss 0.091029\n",
            "Epoch 16 | Train loss: 0.093704 | Val loss 0.092589\n",
            "Epoch 17 | Train loss: 0.072145 | Val loss 0.100989\n",
            "Epoch 18 | Train loss: 0.079800 | Val loss 0.075945\n",
            "Epoch 19 | Train loss: 0.063704 | Val loss 0.071103\n",
            "Epoch 20 | Train loss: 0.057372 | Val loss 0.084803\n",
            "Epoch 21 | Train loss: 0.057947 | Val loss 0.070500\n",
            "Epoch 22 | Train loss: 0.053946 | Val loss 0.067825\n",
            "Epoch 23 | Train loss: 0.043488 | Val loss 0.073677\n",
            "Epoch 24 | Train loss: 0.042706 | Val loss 0.074659\n",
            "Epoch 25 | Train loss: 0.042472 | Val loss 0.071008\n",
            "\n",
            "Training model: hidden_size=16, dataset=large\n",
            "Epoch 1 | Train loss: 2.614732 | Val loss 2.240280\n",
            "Epoch 2 | Train loss: 2.067853 | Val loss 1.897426\n",
            "Epoch 3 | Train loss: 1.778429 | Val loss 1.650151\n",
            "Epoch 4 | Train loss: 1.515546 | Val loss 1.302594\n",
            "Epoch 5 | Train loss: 1.059125 | Val loss 0.827766\n",
            "Epoch 6 | Train loss: 0.652628 | Val loss 0.544807\n"
          ]
        }
      ],
      "source": [
        "configs = [\n",
        "    (16, 'small', encdec_ds_small),\n",
        "    (16, 'large', encdec_ds_large),\n",
        "    (32, 'small', encdec_ds_small),\n",
        "    (32, 'large', encdec_ds_large)\n",
        "]\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for hidden_size, dataset_name, dataset in configs:\n",
        "    print(f\"\\nTraining model: hidden_size={hidden_size}, dataset={dataset_name}\")\n",
        "\n",
        "    model = TransformerEncoderDecoder(\n",
        "        vocab_size=len(dataset.idx_to_token),\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=4\n",
        "    )\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-2)\n",
        "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "    results = train(\n",
        "        model = model,\n",
        "        dataset = dataset,\n",
        "        num_epochs = 25,\n",
        "        optimizer = optimizer,\n",
        "        scheduler = scheduler,\n",
        "        device = device,\n",
        "        batch_size = 32 if dataset_name == \"small\" else 256\n",
        "    )\n",
        "    all_results[(hidden_size, dataset_name)] = results\n",
        "\n",
        "save_loss_comparison_by_dataset(all_results)\n",
        "save_loss_comparison_by_hidden(all_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8426bdc5-f469-47a2-a7fb-cb66b25341db",
      "metadata": {
        "id": "8426bdc5-f469-47a2-a7fb-cb66b25341db"
      },
      "source": [
        "# Part 2: Decoder-only Transformers and Multi-Head Attention\n",
        "\n",
        "In this section, you'll train transformers that only use a decoder instead of a dual encoder-decoder architecture. You'll also implement and experiment with multi-head attention. Finally, you'll compare increasing parameter count with multi-head attention vs. increasing the parameter count with a larger hidden dimensionality."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J2D9jjd5EPyA",
      "metadata": {
        "id": "J2D9jjd5EPyA"
      },
      "source": [
        "## Step 1: Implement the Decoder-Only Forward Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a2b6e1a-12f8-4a95-9115-b402dc946b3a",
      "metadata": {
        "id": "2a2b6e1a-12f8-4a95-9115-b402dc946b3a"
      },
      "outputs": [],
      "source": [
        "class DecoderOnlyBlock(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.norm1 = LayerNorm(hidden_size)\n",
        "        self.norm2 = LayerNorm(hidden_size)\n",
        "        self.attn = CausalAttention(hidden_size)\n",
        "        self.mlp = MLP(hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        context, attention = self.attn(self.norm1(x))\n",
        "        x = x + context\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x, attention\n",
        "\n",
        "class DecoderTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tok_emb = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.pos_emb = PositionalEncoding(hidden_size)\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [DecoderOnlyBlock(hidden_size) for i in range(num_layers)]\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "        self.head.weight = self.tok_emb.weight\n",
        "        # weight tie, the final layer reuses the weights of the embedding layers\n",
        "        # this reduces the total number of parameters\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the Transformer decoder.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        # pos =\n",
        "        # x =\n",
        "        # self_attn = []\n",
        "\n",
        "        # for block in self.blocks:\n",
        "        #     ...\n",
        "        #\n",
        "        # unormalized_scores =\n",
        "\n",
        "        pos = torch.arange(0, inputs.shape[1], device=inputs.device)\n",
        "        x = self.tok_emb(inputs) + self.pos_emb(pos)\n",
        "        self_attn = []\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x, attention = block(x)\n",
        "            self_attn.append(attention)\n",
        "\n",
        "        unormalized_scores = self.head(x)\n",
        "\n",
        "        return unormalized_scores, {'self_attn': self_attn}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcc8e345-5264-4dad-963b-753691166160",
      "metadata": {
        "id": "fcc8e345-5264-4dad-963b-753691166160"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'DecoderDataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m decoder_ds_small = \u001b[43mDecoderDataset\u001b[49m(data_path, \u001b[33m'\u001b[39m\u001b[33mpig_latin_small\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m decoder_ds_large = DecoderDataset(data_path, \u001b[33m'\u001b[39m\u001b[33mpig_latin_large\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'DecoderDataset' is not defined"
          ]
        }
      ],
      "source": [
        "decoder_ds_small = DecoderDataset(data_path, 'pig_latin_small')\n",
        "decoder_ds_large = DecoderDataset(data_path, 'pig_latin_large')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cfe222c-040d-48df-94c8-98b8926f8428",
      "metadata": {
        "id": "5cfe222c-040d-48df-94c8-98b8926f8428"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset sample:\")\n",
        "print(\"Input:\", decoder_ds_small.to_token(decoder_ds_small[0][0].tolist()))\n",
        "print(\"Target:\", decoder_ds_small.to_token(decoder_ds_small[0][1].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5F_IOz8TB4Jn",
      "metadata": {
        "id": "5F_IOz8TB4Jn"
      },
      "source": [
        "## Step 2: Format for decoder-only generation.\n",
        "\n",
        "Implement the missing line in `decoder_generate` to create an input string that looks like:\n",
        "\n",
        "```\n",
        "<SOS> input sequence <EOP>\n",
        "```\n",
        "You should use the `dataset.start_idx`, `dataset.eop_idx` attributes and the `dataset.to_idx()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "321ce6f3-8397-4faf-a012-f79a7089f19a",
      "metadata": {
        "id": "321ce6f3-8397-4faf-a012-f79a7089f19a"
      },
      "outputs": [],
      "source": [
        "def decoder_generate(input, model, dataset, max_len=20):\n",
        "    gen_string = \"\"\n",
        "    # idx =\n",
        "    idx = torch.tensor([idx]).to(device)\n",
        "\n",
        "    for i in range(max_len):\n",
        "        logits = model(idx)[0]\n",
        "\n",
        "        token_idx = logits[0, -1:].argmax(dim=-1)\n",
        "        idx = torch.cat([idx, token_idx.unsqueeze(0)], dim=1)\n",
        "\n",
        "        if token_idx.item() == dataset.end_idx:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = gen_string + dataset.idx_to_token[token_idx.item()]\n",
        "    return gen_string\n",
        "\n",
        "def decoder_translate(sentence, model, dataset):\n",
        "    words = sentence.split()\n",
        "    return \" \".join([decoder_generate(w, model, dataset) for w in words])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ipv3kQx2Il4A",
      "metadata": {
        "id": "Ipv3kQx2Il4A"
      },
      "source": [
        "## Step 3: Train the model\n",
        "\n",
        "We'll now train a decoder only model and test it out on an example. Use the output of this to guide your answer for question 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dca96f6-2b65-46ef-95b3-369b4bf11fe6",
      "metadata": {
        "id": "4dca96f6-2b65-46ef-95b3-369b4bf11fe6"
      },
      "outputs": [],
      "source": [
        "model = DecoderTransformer(\n",
        "    vocab_size = len(decoder_ds_small.idx_to_token),\n",
        "    hidden_size = 16,\n",
        "    num_layers = 4\n",
        ")\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-2)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "results = train(\n",
        "    model = model,\n",
        "    dataset = decoder_ds_small,\n",
        "    num_epochs = 25,\n",
        "    optimizer = optimizer,\n",
        "    scheduler = scheduler,\n",
        "    device = device,\n",
        "    batch_size = 32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6b7ad41-06c5-4791-8f2f-2aaf296f9790",
      "metadata": {
        "id": "b6b7ad41-06c5-4791-8f2f-2aaf296f9790"
      },
      "outputs": [],
      "source": [
        "decoder_translate(\"the air conditioning is working\", model, decoder_ds_small)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f71bab6f-14c9-43f5-a94a-b8d7a7765930",
      "metadata": {
        "id": "f71bab6f-14c9-43f5-a94a-b8d7a7765930"
      },
      "source": [
        "## Step 4: Implement Multi-Head Attention\n",
        "\n",
        "In this next step, you'll implement multi-head attention. In this setting, we'll have the dimensionality for each of the heads be (total hidden dimensionality)/(number of heads). We'll use a projection head to combine the outputs of these different attention heads after concatenating their outputs together (which can also be achieved using tensor reshaping)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76594ab7-6ffe-44ed-83c9-d47e44995639",
      "metadata": {
        "id": "76594ab7-6ffe-44ed-83c9-d47e44995639"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# TO DO:\n",
        "\n",
        "def multihead_scaled_dot_attention(q, k, v, mask=0):\n",
        "    \"\"\" Compute multi-head dot product attention.\n",
        "      q : shape (batch, num_heads, k, hidden_size)\n",
        "      k : shape (batch, num_heads, seq_len, hidden_size)\n",
        "      v : shape (batch, num_heads, seq_len, hidden_size)\n",
        "\n",
        "    returns:\n",
        "        context   : shape (batch, num_heads, k, hidden_size)\n",
        "        attention : shape (batch, num_heads, k, seq_len)\n",
        "    \"\"\"\n",
        "    # Hint: if you implemented scaled_dot_attention well, the code for this should be the exact same.\n",
        "\n",
        "    # ------------\n",
        "    # FILL THIS IN\n",
        "    # ------------\n",
        "    # unnorm_attn =\n",
        "    # masked_unnorm_attn =\n",
        "    # attention =\n",
        "    # context =\n",
        "    return context, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "416c5a3c-ca9d-462a-949a-43cb1bbad5df",
      "metadata": {
        "id": "416c5a3c-ca9d-462a-949a-43cb1bbad5df"
      },
      "outputs": [],
      "source": [
        "class MultiheadCausalAttention(Attention):\n",
        "    def __init__(self, hidden_size, num_heads):\n",
        "        super().__init__(hidden_size)\n",
        "        # self.Q, self.K, self.V already defined as nn.Linear(hidden_size, hidden_size)\n",
        "        self.num_heads = num_heads\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        # if self.num_heads > 1:\n",
        "        #   self.proj = nn.Linear( ,  , bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        \"\"\" Compute causal multi-head dot product attention from an input sequence.\n",
        "          x : shape ( batch, seq_len, hidden_size)\n",
        "          k : shape (batch, num_heads, seq_len, hidden_size)\n",
        "          v : shape (batch, num_heads, seq_len, hidden_size)\n",
        "\n",
        "        returns:\n",
        "            context : shape ( batch, seq_len, hidden_size)\n",
        "            attention : shape (batch, num_heads, k, seq_len)\n",
        "        \"\"\"\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        # unnorm_attn =\n",
        "        # masked_unnorm_attn =\n",
        "        # attention =\n",
        "        # context =\n",
        "\n",
        "        # B, T, C = x.size()\n",
        "        # q = self.Q(x).view( ,  ,  ,  ).transpose(1, 2)\n",
        "        # k = self.K(x).view( ,  ,  ,  ).transpose(1, 2)\n",
        "        # v = self.V(x).view( ,  ,  ,  ).transpose(1, 2)\n",
        "\n",
        "        # mask =\n",
        "\n",
        "        # context, attention = multihead_scaled_dot_attention(q, k, v, mask)\n",
        "\n",
        "        # Hint: you may find the .transpose(), .contiguous(), and .view()\n",
        "        # methods helpful for creating the reshaped context.\n",
        "\n",
        "        # reshaped_context =\n",
        "\n",
        "\n",
        "        # Only perform a projection if the number of heads is more than 1.\n",
        "        if self.num_heads > 1:\n",
        "\n",
        "          return self.proj(reshaped_context), attention\n",
        "\n",
        "        else:\n",
        "\n",
        "          return reshaped_context, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3c465e0-95af-4875-a355-b205fbaa9c0a",
      "metadata": {
        "id": "c3c465e0-95af-4875-a355-b205fbaa9c0a"
      },
      "outputs": [],
      "source": [
        "class MultiheadDecoderBlock(DecoderOnlyBlock):\n",
        "    def __init__(self, hidden_size, num_heads):\n",
        "        super().__init__(hidden_size)\n",
        "        self.attn = MultiheadCausalAttention(hidden_size, num_heads)\n",
        "\n",
        "\n",
        "class MultiheadDecoderTransformer(DecoderTransformer):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads):\n",
        "        super().__init__(vocab_size, hidden_size, num_layers)\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [MultiheadDecoderBlock(hidden_size, num_heads) for i in range(num_layers)]\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gVZndUxDOC0f",
      "metadata": {
        "id": "gVZndUxDOC0f"
      },
      "source": [
        "## Step 5: Compare Parameter Usage Methods\n",
        "\n",
        "Here, you'll train 5 models with varying numbers of attention heads and hidden dimensionalities. You'll compare their parameter efficiency. Start by implementing the `calc_parameters_decoder` method, and then run the code to train the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ei-u1xyjbz0-",
      "metadata": {
        "id": "ei-u1xyjbz0-"
      },
      "outputs": [],
      "source": [
        "def calc_parameters_decoder(model):\n",
        "  # ------------\n",
        "  # FILL THIS IN\n",
        "  # ------------\n",
        "  # params = TODO # embedding\n",
        "  # params +=  TODO # QKV linear projections\n",
        "  # if model.num_heads > 1:\n",
        "  #   params += TODO # attention linear projection\n",
        "  # params += TODO # MLP linear\n",
        "  # params += TODO # layer norms Hint: LayerNorm(x) uses x parameters\n",
        "\n",
        "  # Hint: Read the code carefully:\n",
        "  # params += TODO # head \n",
        "\n",
        "  return params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lyrc0QpxN781",
      "metadata": {
        "id": "lyrc0QpxN781"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "configs = [\n",
        "    (16, 4, 4, 64), #(hidden_size, num_heads, num_layers, batch_size)\n",
        "    (16, 2, 4, 64),\n",
        "    (16, 1, 4, 64),\n",
        "    (24, 1, 4, 64),\n",
        "    (32, 1, 4, 64),\n",
        "]\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for hidden_size, num_heads, num_layers, batch_size in configs:\n",
        "    print(f\"\\nTraining model: hidden_size={hidden_size}, num_heads={num_heads}, num_layers={num_layers}\")\n",
        "\n",
        "    model = MultiheadDecoderTransformer(\n",
        "        vocab_size = len(decoder_ds_large.idx_to_token),\n",
        "        hidden_size = hidden_size,\n",
        "        num_layers = num_layers,\n",
        "        num_heads = num_heads\n",
        "    )\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=5e-3)\n",
        "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "    results = train(\n",
        "        model = model,\n",
        "        dataset = decoder_ds_large,\n",
        "        num_epochs = 25,\n",
        "        optimizer = optimizer,\n",
        "        scheduler = scheduler,\n",
        "        device = device,\n",
        "        batch_size = batch_size\n",
        "    )\n",
        "\n",
        "    parameters = calc_parameters_decoder(model)\n",
        "    x_points = np.linspace(0, len(results['val_loss']), len(results['val_loss']))\n",
        "    plt.plot(\n",
        "        x_points, results['val_loss'], label=\"heads = {}, dim={}, params={}\".format(\n",
        "            num_heads, hidden_size, parameters\n",
        "        ))\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Validation Loss\")\n",
        "plt.title('Compute Optimal Models')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RlH5XVFjW4oq",
      "metadata": {
        "id": "RlH5XVFjW4oq"
      },
      "outputs": [],
      "source": [
        "plt.clf()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bDMIftx1N85z",
      "metadata": {
        "id": "bDMIftx1N85z"
      },
      "source": [
        "# Part 3: IsoFLOP Profiling and Scaling Laws\n",
        "In this final section, you'll compare different architectures based on their compute efficiency. You will also determine the optimal number of parameters and tokens for a fixed compute budget.\n",
        "\n",
        "While this section does not require any additional code from you, **use the output of it to guide your written responses**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y846ApAZkFDF",
      "metadata": {
        "id": "Y846ApAZkFDF"
      },
      "outputs": [],
      "source": [
        "from utils import (\n",
        "    plot_scaling_law,\n",
        "    plot_scaling_law_poly,\n",
        "    interpolate,\n",
        "    plot_flops_tokens,\n",
        "    plot_isoflop,\n",
        "    plot_flops_params\n",
        ")\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dTLS2J_PTUS4",
      "metadata": {
        "id": "dTLS2J_PTUS4"
      },
      "source": [
        "## Step 1: Train models, Compare FLOPS vs Validation Loss\n",
        "\n",
        "We'll inspect the compute efficiency of different models by plotting their validation loss against the number of FLOPs used in training up to that point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gSkiBVS7C3Mp",
      "metadata": {
        "id": "gSkiBVS7C3Mp"
      },
      "outputs": [],
      "source": [
        "\n",
        "configs = [\n",
        "    (40, 2, 4, 256), #(hidden_size, num_heads, num_layers, batch_size)\n",
        "    (32, 2, 4, 128),\n",
        "    (24, 2, 4, 64),\n",
        "    (16, 1, 4, 64),\n",
        "    (16, 1, 3, 64),\n",
        "    (12, 1, 3, 64),\n",
        "    (12, 1, 2, 64),\n",
        "]\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for hidden_size, num_heads, num_layers, batch_size in configs:\n",
        "    print(f\"\\nTraining model: hidden_size={hidden_size}, num_heads={num_heads}, num_layers={num_layers}\")\n",
        "\n",
        "    model = MultiheadDecoderTransformer(\n",
        "        vocab_size = len(decoder_ds_large.idx_to_token),\n",
        "        hidden_size = hidden_size,\n",
        "        num_layers = num_layers,\n",
        "        num_heads = num_heads\n",
        "    )\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=5e-3)\n",
        "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "    results = train(\n",
        "        model = model,\n",
        "        dataset = decoder_ds_large,\n",
        "        num_epochs = 25,\n",
        "        optimizer = optimizer,\n",
        "        scheduler = scheduler,\n",
        "        device = device,\n",
        "        batch_size = batch_size\n",
        "    )\n",
        "\n",
        "    parameters = calc_parameters_decoder(model)\n",
        "    all_results[parameters] = results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6fN6IpUECFH",
      "metadata": {
        "id": "c6fN6IpUECFH"
      },
      "outputs": [],
      "source": [
        "plot_scaling_law(all_results)\n",
        "\n",
        "plot_scaling_law_poly(all_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x9I1zfQOUB-l",
      "metadata": {
        "id": "x9I1zfQOUB-l"
      },
      "source": [
        "## Step 2: Optimal Number of Parameters\n",
        "\n",
        "Using the previously created training runs, we'll fit quadratic functions to the (parameter count, validation loss) pairs to estimate the optimal number of parameters for a given compute budget. We'll then fit a line to this set of optimal parameters to estimate the optimal number of parameters for a compute budget of 1e15."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rvH920tAPo9E",
      "metadata": {
        "id": "rvH920tAPo9E"
      },
      "outputs": [],
      "source": [
        "def find_optim_params(params, loss):\n",
        "    x = np.array(params)\n",
        "    y = np.array(loss)\n",
        "\n",
        "    p = np.polyfit(np.log10(x), y, 2)\n",
        "    optimal_log_params = -(p[1]) / (2 * p[0])\n",
        "    optimal_params = 10**(optimal_log_params)\n",
        "\n",
        "    return optimal_params\n",
        "\n",
        "\n",
        "target_flops = [8e9, 16e9, 32e9, 64e9, 128e9]\n",
        "colors = ['#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00']\n",
        "params_vs_loss = []\n",
        "optimal_params = []\n",
        "\n",
        "for target in target_flops:\n",
        "  parameters = []\n",
        "  val_loss = []\n",
        "\n",
        "  for params, results in all_results.items():\n",
        "    loss = interpolate(results['val_loss'], results['flops'], target, deg=4)\n",
        "    # interpolate returns None if target is out of range\n",
        "    if loss != None:\n",
        "      parameters.append(params)\n",
        "      val_loss.append(loss)\n",
        "\n",
        "  sorted_indices = torch.argsort(torch.tensor(parameters))\n",
        "  parameters = torch.tensor(parameters)[sorted_indices]\n",
        "  val_loss = torch.tensor(val_loss)[sorted_indices]\n",
        "\n",
        "  optimal_params.append(find_optim_params(parameters, val_loss))\n",
        "  params_vs_loss.append((parameters.tolist(), val_loss.tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nRNcdDOS-_Ns",
      "metadata": {
        "id": "nRNcdDOS-_Ns"
      },
      "outputs": [],
      "source": [
        "plot_isoflop(target_flops, colors, params_vs_loss, optimal_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aFpyymiuS3z1",
      "metadata": {
        "id": "aFpyymiuS3z1"
      },
      "outputs": [],
      "source": [
        "plot_flops_params(target_flops, optimal_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yvRVfbu9VdnZ",
      "metadata": {
        "id": "yvRVfbu9VdnZ"
      },
      "source": [
        "## Step 3: Optimal Number of Tokens\n",
        "\n",
        "Finally, we'll perform a similar analysis to the previous step, only this time determining the optimal number of tokens for a budget of 1e15 flops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ByN1dr2sT0WS",
      "metadata": {
        "id": "ByN1dr2sT0WS"
      },
      "outputs": [],
      "source": [
        "plot_flops_tokens(target_flops, optimal_params, all_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "glu9SEJDoc8o",
      "metadata": {
        "id": "glu9SEJDoc8o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
